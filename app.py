import os
import re
import time
import random
import datetime
from typing import List, Optional, Dict, Tuple

import pandas as pd
import streamlit as st
from pydantic import BaseModel, Field

from google import genai
from google.genai import types
from google.genai import errors as genai_errors

import streamlit.components.v1 as components


# =========================
# API / Ads keys (ç›´æ›¸ãç¦æ­¢)
# =========================
def get_secret(name: str, default=None):
    try:
        return st.secrets.get(name, default)
    except Exception:
        return default


def load_api_key() -> Optional[str]:
    return get_secret("GEMINI_API_KEY") or os.environ.get("GEMINI_API_KEY")


API_KEY = load_api_key()
if not API_KEY:
    st.error("APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚.streamlit/secrets.toml ã‹ ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()


ADSENSE_CLIENT = get_secret("ADSENSE_CLIENT") or os.environ.get("ADSENSE_CLIENT")  # ä¾‹: ca-pub-xxxxxxxx
ADSENSE_SLOT_TOP = get_secret("ADSENSE_SLOT_TOP") or os.environ.get("ADSENSE_SLOT_TOP")  # ä¾‹: 1234567890
ADSENSE_SLOT_BOTTOM = get_secret("ADSENSE_SLOT_BOTTOM") or os.environ.get("ADSENSE_SLOT_BOTTOM")


@st.cache_resource
def get_client():
    return genai.Client(api_key=API_KEY)


MODEL_NAME = "gemini-2.5-flash"  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸ã°ã›ãªã„ï¼ˆå›ºå®šï¼‰


# =========================
# æ—¥ä»˜ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæ¬¡ã®åœŸæ›œï¼‰
# =========================
def default_race_date() -> datetime.date:
    today = datetime.date.today()
    wd = today.weekday()  # Mon=0 ... Sun=6
    if wd in (5, 6):  # Sat/Sun
        return today
    days_ahead = (5 - wd) % 7
    if days_ahead == 0:
        days_ahead = 7
    return today + datetime.timedelta(days=days_ahead)


# =========================
# Structured Output schema
# =========================
class Pick(BaseModel):
    æ ç•ª: int = Field(..., ge=1, le=8)
    é¦¬ç•ª: int = Field(..., ge=1, le=18)
    é¦¬å: str = Field(..., min_length=1)

    äººæ°—æƒ³å®š: Optional[str] = None
    è©•ä¾¡: str = Field(description="S/A/B/C ã®ã„ãšã‚Œã‹")

    èƒ½åŠ›: int = Field(ge=0, le=100)
    é©æ€§: int = Field(ge=0, le=100)
    å±•é–‹: int = Field(ge=0, le=100)
    èª¿å­: int = Field(ge=0, le=100)
    ä¸ç¢ºå®Ÿæ€§: int = Field(ge=0, le=100, description="é«˜ã„ã»ã©æƒ…å ±ä¸è¶³")

    æ€è€ƒã«ã‚ˆã‚‹åˆ†æçµæœ: str
    æ‡¸å¿µç‚¹: str


class PredictionResult(BaseModel):
    picks: List[Pick]


# =========================
# Retry / utils
# =========================
def parse_retry_seconds(e: Exception) -> Optional[float]:
    s = str(e)
    m = re.search(r"Please retry in ([0-9.]+)s", s)
    if m:
        return float(m.group(1))
    m = re.search(r"'retryDelay': '([0-9.]+)s'", s)
    if m:
        return float(m.group(1))
    return None


def is_quota_zero_message(e: Exception) -> bool:
    s = str(e)
    return ("limit: 0" in s) and ("Quota exceeded" in s)


def call_with_retry(fn, max_attempts=6, base_sleep=0.8, max_sleep=20.0):
    last = None
    for attempt in range(1, max_attempts + 1):
        try:
            return fn()
        except (genai_errors.ServerError, genai_errors.ClientError) as e:
            last = e
            msg = str(e)
            retryable = (
                "503" in msg or "UNAVAILABLE" in msg
                or "429" in msg or "RESOURCE_EXHAUSTED" in msg
            )
            if (not retryable) or attempt == max_attempts:
                raise

            delay = parse_retry_seconds(e)
            if delay is not None:
                sleep = min(max_sleep, delay + random.random())
            else:
                sleep = min(max_sleep, base_sleep * (2 ** (attempt - 1)))
                sleep *= (0.7 + 0.6 * random.random())
            time.sleep(sleep)
    raise last


def extract_sources_from_response(response) -> List[dict]:
    sources = []
    try:
        cand = response.candidates[0]
        gm = getattr(cand, "grounding_metadata", None)
        if not gm:
            return sources
        chunks = getattr(gm, "grounding_chunks", None) or []
        for ch in chunks:
            web = getattr(ch, "web", None)
            if web and getattr(web, "uri", None):
                title = getattr(web, "title", None) or web.uri
                sources.append({"title": title, "uri": web.uri})
    except Exception:
        pass
    return sources


def uniq_keep_order(xs: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in xs:
        if x and x not in seen:
            seen.add(x)
            out.append(x)
    return out


def normalize_weights(w: Dict[str, float]) -> Dict[str, float]:
    s = sum(w.values())
    if s <= 0:
        n = len(w)
        return {k: 1.0 / n for k in w}
    return {k: v / s for k, v in w.items()}


def compute_total_score(p: Pick, w: Dict[str, float]) -> float:
    return (
        w["èƒ½åŠ›"] * p.èƒ½åŠ›
        + w["é©æ€§"] * p.é©æ€§
        + w["å±•é–‹"] * p.å±•é–‹
        + w["èª¿å­"] * p.èª¿å­
        + w["ç¢ºåº¦"] * (100 - p.ä¸ç¢ºå®Ÿæ€§)
    )


def grade_rank(g: str) -> int:
    # ã‚½ãƒ¼ãƒˆç”¨ï¼šSãŒæœ€å¼·
    return {"S": 0, "A": 1, "B": 2, "C": 3}.get(g, 9)


# =========================
# Ads (AdSenseæƒ³å®š)
# =========================
def render_adsense(slot: Optional[str], height: int = 120):
    """
    AdSenseã‚’å…¥ã‚Œã‚‹æ ï¼ˆclient/slotãŒæœªè¨­å®šãªã‚‰ä½•ã‚‚å‡ºã•ãªã„ï¼‰
    NOTE: å¯©æŸ»é€šéå¾Œã«æœ¬ç•ªã‚³ãƒ¼ãƒ‰ã§å‹•ãæƒ³å®šã€‚å¯©æŸ»å‰ã¯å‡ºã•ãªã„æ–¹ãŒç„¡é›£ã€‚
    """
    if not ADSENSE_CLIENT or not slot:
        return

    html = f"""
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client={ADSENSE_CLIENT}"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="{ADSENSE_CLIENT}"
     data-ad-slot="{slot}"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({{}});
</script>
"""
    components.html(html, height=height)


# =========================
# Entry (å‡ºé¦¬è¡¨) extraction
# =========================
ENTRY_BLOCK_RE = re.compile(r"<ENTRY>\s*(.*?)\s*</ENTRY>", re.S)


def parse_entry_block(text: str) -> List[Dict[str, object]]:
    m = ENTRY_BLOCK_RE.search(text)
    if not m:
        return []

    body = m.group(1).strip()
    lines = [ln.strip() for ln in body.splitlines() if ln.strip()]

    entries = []
    for ln in lines:
        if ln.startswith("æ ç•ª"):
            continue

        parts = re.split(r"[,\uFF0C\t]+|\s{2,}", ln)
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) < 3:
            m2 = re.match(r"^\s*(\d+)\s+(\d+)\s+(.+?)\s*$", ln)
            if not m2:
                continue
            parts = [m2.group(1), m2.group(2), m2.group(3)]

        try:
            waku = int(parts[0])
            uma = int(parts[1])
        except Exception:
            continue

        name = parts[2].strip()
        if not name:
            continue

        entries.append({"æ ç•ª": waku, "é¦¬ç•ª": uma, "é¦¬å": name})

    return entries


def search_candidate_urls(query: str) -> Tuple[List[str], List[dict]]:
    client = get_client()

    prompt = f"""
æ¬¡ã®ãƒ¬ãƒ¼ã‚¹ã®ã€Œå‡ºé¦¬è¡¨ï¼ˆæ ç•ªãƒ»é¦¬ç•ªãƒ»é¦¬åãŒè¼‰ã£ã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ï¼‰ã€ã®URLå€™è£œã‚’æ¢ã—ã¦ãã ã•ã„ã€‚
å„ªå…ˆ: netkeiba ã® shutubaï¼ˆrace.netkeiba.com/race/shutuba.html?race_id=...ï¼‰ãªã©ã€‚

å¯¾è±¡ãƒ¬ãƒ¼ã‚¹: {query}

å‡ºåŠ›ã¯æ–‡ç« ã§OKï¼ˆURLã‚’å«ã‚ã‚‹ï¼‰ã€‚ãŸã ã—æ­£ç¢ºãªURLã‚’å«ã‚ã¦ãã ã•ã„ã€‚
"""
    cfg = types.GenerateContentConfig(
        tools=[{"google_search": {}}],
        temperature=0.0,
    )

    resp = call_with_retry(lambda: client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config=cfg,
    ))

    sources = extract_sources_from_response(resp)

    urls = [s["uri"] for s in sources if s.get("uri")]
    urls += re.findall(r"https?://[^\s)ã€‘]+", resp.text or "")
    urls = uniq_keep_order(urls)
    return urls, sources


def rank_urls(urls: List[str]) -> List[str]:
    def score(u: str) -> int:
        u2 = u.lower()
        sc = 0
        if "race.netkeiba.com" in u2 and "shutuba" in u2:
            sc += 100
        if "race_id=" in u2:
            sc += 30
        if "shutuba" in u2:
            sc += 20
        return sc

    return sorted(urls, key=score, reverse=True)


def extract_entry_from_url(url: str) -> List[Dict[str, object]]:
    client = get_client()

    cfg = types.GenerateContentConfig(
        tools=[{"url_context": {}}],
        temperature=0.0,
    )

    prompt = f"""
æ¬¡ã®URLã®å†…å®¹ã‚’èª­ã¿å–ã‚Šã€å‡ºé¦¬è¡¨ï¼ˆæ ç•ªãƒ»é¦¬ç•ªãƒ»é¦¬åï¼‰ã ã‘ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€å³å®ˆã€‘
- é¦¬åã¯ãƒšãƒ¼ã‚¸ã®è¡¨è¨˜ã‚’ã€Œãã®ã¾ã¾ã€ã‚³ãƒ”ãƒ¼ã€‚ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼/èª¬æ˜æ–‡/è£…é£¾ã¯ç¦æ­¢ã€‚
- æ¨æ¸¬ã§åŸ‹ã‚ãªã„ã€‚ä¸æ˜ãªã‚‰ãã®è¡Œã¯å‡ºã•ãªã„ã€‚
- å‡ºåŠ›ã¯å¿…ãšæ¬¡ã®å½¢å¼ã®ã¿ï¼ˆã“ã‚Œä»¥å¤–ã®æ–‡å­—ã¯ç¦æ­¢ï¼‰:

<ENTRY>
æ ç•ª,é¦¬ç•ª,é¦¬å
1,1,é¦¬å
...
</ENTRY>

URL: {url}
"""
    resp = call_with_retry(lambda: client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config=cfg,
    ))

    return parse_entry_block(resp.text or "")


def get_entry_list(query: str, entry_url_override: str = "") -> Tuple[List[Dict[str, object]], str, List[dict]]:
    if entry_url_override.strip():
        url = entry_url_override.strip()
        entries = extract_entry_from_url(url)
        if len(entries) >= 5:
            return entries, url, []
        raise ValueError("è²¼ã‚Šä»˜ã‘ãŸå‡ºé¦¬è¡¨URLã‹ã‚‰æ ç•ª/é¦¬ç•ª/é¦¬åã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®å‡ºé¦¬è¡¨URLã‚’è²¼ã£ã¦ãã ã•ã„ã€‚")

    urls, sources = search_candidate_urls(query)
    urls_ranked = rank_urls(urls)

    for url in urls_ranked[:8]:
        entries = extract_entry_from_url(url)
        if len(entries) >= 5:
            return entries, url, sources

    raise ValueError("å‡ºé¦¬è¡¨(ENTRYãƒ–ãƒ­ãƒƒã‚¯)ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜/ç«¶é¦¬å ´/Rã‚’ç¢ºèªã—ã€å‡ºé¦¬è¡¨URLã‚’è²¼ã‚‹ã‹ã€æ—¥ä»˜ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


# =========================
# Prediction flow
# =========================
def build_research_memo(query: str, entry_url: str, entries: List[Dict[str, object]]) -> Tuple[str, List[dict]]:
    client = get_client()

    horses_compact = " / ".join([f"{e['é¦¬å']}({e['æ ç•ª']}-{e['é¦¬ç•ª']})" for e in entries])

    cfg = types.GenerateContentConfig(
        tools=[{"url_context": {}}, {"google_search": {}}],
        temperature=0.3,
    )

    prompt = f"""
ã‚ãªãŸã¯è«–ç†çš„ãªç«¶é¦¬åˆ†æå®˜ã§ã™ã€‚
æ¬¡ã®ãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€æœ€æ–°æƒ…å ±ã‚’èª¿æŸ»ã—ã¦ã€Œåˆ†æãƒ¡ãƒ¢ã€ã‚’ä½œã£ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã€‘
{query}

ã€å‡ºé¦¬è¡¨URLï¼ˆæ ç•ªãƒ»é¦¬ç•ªãƒ»é¦¬åã®æ ¹æ‹ ï¼‰ã€‘
{entry_url}

ã€å‡ºèµ°é¦¬ï¼ˆçŸ­ç¸®ï¼‰ã€‘
{horses_compact}

ã€å‡ºåŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã€‘
- ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆå¤©æ°—/é¦¬å ´/ã‚³ãƒ¼ã‚¹å‚¾å‘/æ ã®æœ‰åˆ©ä¸åˆ©ï¼‰
- ãƒšãƒ¼ã‚¹ã¨å±•é–‹æƒ³å®šï¼ˆè„šè³ªåˆ†å¸ƒï¼‰
- æœ‰åŠ›é¦¬å€™è£œï¼ˆæ ¹æ‹ ãƒ»ä¸å®‰è¦ç´ ï¼‰
- æƒ…å ±ä¸è¶³ï¼ˆå–ã‚Œãªã„æƒ…å ±ãŒã‚ã‚Œã°æ˜ç¤ºï¼‰
"""
    resp = call_with_retry(lambda: client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config=cfg,
    ))
    sources = extract_sources_from_response(resp)
    return (resp.text or ""), sources


def validate_picks_against_entries(picks: List[Pick], entries: List[Dict[str, object]]) -> Tuple[bool, str]:
    name_to_nums = {e["é¦¬å"]: (e["æ ç•ª"], e["é¦¬ç•ª"]) for e in entries}
    for p in picks:
        if p.é¦¬å not in name_to_nums:
            return False, f"é¦¬åãŒå‡ºé¦¬è¡¨ã«å­˜åœ¨ã—ã¾ã›ã‚“: {p.é¦¬å}"
        w, n = name_to_nums[p.é¦¬å]
        if p.æ ç•ª != w or p.é¦¬ç•ª != n:
            return False, f"æ ç•ª/é¦¬ç•ªãŒå‡ºé¦¬è¡¨ã¨ä¸€è‡´ã—ã¾ã›ã‚“: {p.é¦¬å} (å‡ºåŠ›={p.æ ç•ª}-{p.é¦¬ç•ª}, æ­£={w}-{n})"
        if p.è©•ä¾¡ not in ["S", "A", "B", "C"]:
            return False, f"è©•ä¾¡ãŒä¸æ­£ã§ã™: {p.é¦¬å} è©•ä¾¡={p.è©•ä¾¡}"
    return True, ""


def format_picks_to_json(entries: List[Dict[str, object]], memo: str, weights: Dict[str, float]) -> PredictionResult:
    client = get_client()

    allowed = [{"æ ç•ª": e["æ ç•ª"], "é¦¬ç•ª": e["é¦¬ç•ª"], "é¦¬å": e["é¦¬å"]} for e in entries]

    prompt = f"""
ã‚ãªãŸã¯è«–ç†çš„ãªç«¶é¦¬åˆ†æå®˜ã§ã™ã€‚
æ¬¡ã®ã€Œå‡ºé¦¬è¡¨ãƒªã‚¹ãƒˆã€ã«è¼‰ã£ã¦ã„ã‚‹é¦¬ã ã‘ã‹ã‚‰ã€æœ‰åŠ›é¦¬ã‚’5ã€œ6é ­é¸ã³ã€æŒ‡å®šã®JSONå½¢å¼ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ãƒ«ãƒ¼ãƒ«ã€‘
- é¦¬åã¯ã€Œå‡ºé¦¬è¡¨ãƒªã‚¹ãƒˆã€ã®æ–‡å­—åˆ—ã‚’å®Œå…¨ä¸€è‡´ã§ä½¿ç”¨ï¼ˆã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ç¦æ­¢ï¼‰
- æ ç•ª/é¦¬ç•ªã‚‚å‡ºé¦¬è¡¨ãƒªã‚¹ãƒˆã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨
- å‡ºåŠ›ã¯JSONã®ã¿ï¼ˆå‰å¾Œã«æ–‡ç« ç¦æ­¢ï¼‰
- è©•ä¾¡ã¯ S/A/B/C
- ã‚¹ã‚³ã‚¢ã¯ 0ã€œ100 ã®æ•´æ•°

ã€é‡è¦–ãƒã‚¤ãƒ³ãƒˆï¼ˆæ­£è¦åŒ–æ¸ˆï¼‰ã€‘
èƒ½åŠ›={weights["èƒ½åŠ›"]:.3f}, é©æ€§={weights["é©æ€§"]:.3f}, å±•é–‹={weights["å±•é–‹"]:.3f}, èª¿å­={weights["èª¿å­"]:.3f}, ç¢ºåº¦={weights["ç¢ºåº¦"]:.3f}

ã€å‡ºé¦¬è¡¨ãƒªã‚¹ãƒˆã€‘
{allowed}

ã€åˆ†æãƒ¡ãƒ¢ã€‘
{memo}

ã€å‡ºåŠ›JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
{{"picks":[{{"æ ç•ª":1,"é¦¬ç•ª":1,"é¦¬å":"...","äººæ°—æƒ³å®š":"1ç•ªäººæ°—ãªã©","è©•ä¾¡":"S","èƒ½åŠ›":80,"é©æ€§":70,"å±•é–‹":60,"èª¿å­":65,"ä¸ç¢ºå®Ÿæ€§":20,"æ€è€ƒã«ã‚ˆã‚‹åˆ†æçµæœ":"...","æ‡¸å¿µç‚¹":"..."}}]}}
"""
    cfg = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_json_schema=PredictionResult.model_json_schema(),
        temperature=0.2,
    )

    resp = call_with_retry(lambda: client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config=cfg,
    ))
    return PredictionResult.model_validate_json(resp.text)


def get_prediction(entries: List[Dict[str, object]], memo: str, weights: Dict[str, float]) -> List[Pick]:
    last_err = None
    for _ in range(3):
        result = format_picks_to_json(entries, memo, weights)
        ok, reason = validate_picks_against_entries(result.picks, entries)
        if ok:
            return result.picks
        last_err = reason
        memo = memo + f"\n\n[é‡è¦] å‰å›ã®å‡ºåŠ›ã¯ä¸æ­£ã§ã—ãŸã€‚ç†ç”±: {reason}\nå‡ºé¦¬è¡¨ãƒªã‚¹ãƒˆã®é¦¬åãƒ»æ ç•ªãƒ»é¦¬ç•ªã‚’å¿…ãšå®Œå…¨ä¸€è‡´ã§ä½¿ã£ã¦ãã ã•ã„ã€‚"
    raise ValueError(f"å‡ºåŠ›ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ: {last_err}")


def run_prediction(query: str, weights: Dict[str, float], entry_url_override: str = "") -> Tuple[pd.DataFrame, List[dict], str]:
    entries, entry_url, sources1 = get_entry_list(query, entry_url_override=entry_url_override)
    memo, sources2 = build_research_memo(query, entry_url, entries)
    picks = get_prediction(entries, memo, weights)

    df = pd.DataFrame([p.model_dump() for p in picks])
    df["ç·åˆç‚¹"] = [round(compute_total_score(p, weights), 1) for p in picks]

    # è¡¨ç¤ºåï¼ˆè²·ã„ç›®ç”¨ï¼‰
    df["é¦¬ç•ªä»˜ã"] = df.apply(lambda r: f"{int(r['é¦¬ç•ª'])}({r['é¦¬å']})", axis=1)

    # å®‰å®šã‚½ãƒ¼ãƒˆï¼šç·åˆç‚¹ desc â†’ è©•ä¾¡(S>A>B>C) â†’ é¦¬ç•ª asc
    df["è©•ä¾¡é †ä½"] = df["è©•ä¾¡"].apply(grade_rank)
    df = df.sort_values(by=["ç·åˆç‚¹", "è©•ä¾¡é †ä½", "é¦¬ç•ª"], ascending=[False, True, True]).reset_index(drop=True)
    df = df.drop(columns=["è©•ä¾¡é †ä½"])

    sources = sources1 + sources2
    seen = set()
    uniq_sources = []
    for s in sources:
        u = s.get("uri")
        if u and u not in seen:
            seen.add(u)
            uniq_sources.append(s)

    return df, uniq_sources, entry_url


# =========================
# Cacheï¼ˆAPIç„¡é§„æ’ƒã¡é˜²æ­¢ï¼‰
# =========================
def weights_to_key(weights: Dict[str, float]) -> Tuple[float, float, float, float, float]:
    # å°æ•°èª¤å·®ã§åˆ¥ç‰©æ‰±ã„ã«ãªã‚‰ãªã„ã‚ˆã†ä¸¸ã‚ã‚‹
    return (
        round(weights["èƒ½åŠ›"], 4),
        round(weights["é©æ€§"], 4),
        round(weights["å±•é–‹"], 4),
        round(weights["èª¿å­"], 4),
        round(weights["ç¢ºåº¦"], 4),
    )


@st.cache_data(ttl=600, show_spinner=False)  # 10åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def run_prediction_cached(query: str, weights_key: Tuple[float, float, float, float, float], entry_url_override: str):
    weights = {"èƒ½åŠ›": weights_key[0], "é©æ€§": weights_key[1], "å±•é–‹": weights_key[2], "èª¿å­": weights_key[3], "ç¢ºåº¦": weights_key[4]}
    df, sources, entry_url = run_prediction(query=query, weights=weights, entry_url_override=entry_url_override)
    return df, sources, entry_url


# =========================
# Betting helperï¼ˆé¦¬ç•ªä»˜ãï¼‰
# =========================
def make_bets(df_ranked: pd.DataFrame, ticket_type: str, budget_yen: int) -> pd.DataFrame:
    if budget_yen <= 0 or df_ranked.empty:
        return pd.DataFrame(columns=["åˆ¸ç¨®", "è²·ã„ç›®", "é‡‘é¡(å††)"])

    top = df_ranked["é¦¬ç•ªä»˜ã"].tolist()
    unit = 100

    def split_amount(total: int, k: int) -> List[int]:
        total = (total // unit) * unit
        if k <= 0 or total <= 0:
            return []
        base = (total // k // unit) * unit
        amounts = [base] * k
        rem = total - base * k
        i = 0
        while rem >= unit and i < k:
            amounts[i] += unit
            rem -= unit
            i += 1
        return amounts

    bets = []

    if ticket_type in ["é¦¬é€£", "ãƒ¯ã‚¤ãƒ‰", "é¦¬å˜"]:
        if len(top) >= 2:
            axis = top[0]
            opp = top[1:4]
            combos = [f"{axis} â†’ {o}" for o in opp] if ticket_type == "é¦¬å˜" else [f"{axis} - {o}" for o in opp]
            amts = split_amount(budget_yen, len(combos))
            for c, a in zip(combos, amts):
                bets.append({"åˆ¸ç¨®": ticket_type, "è²·ã„ç›®": c, "é‡‘é¡(å††)": a})

    elif ticket_type == "å˜å‹":
        amts = split_amount(budget_yen, 1)
        bets.append({"åˆ¸ç¨®": "å˜å‹", "è²·ã„ç›®": top[0], "é‡‘é¡(å††)": amts[0] if amts else 0})

    elif ticket_type == "è¤‡å‹":
        k = min(2, len(top))
        combos = top[:k]
        amts = split_amount(budget_yen, k)
        for c, a in zip(combos, amts):
            bets.append({"åˆ¸ç¨®": "è¤‡å‹", "è²·ã„ç›®": c, "é‡‘é¡(å††)": a})

    elif ticket_type == "ä¸‰é€£è¤‡":
        if len(top) >= 3:
            a, b = top[0], top[1]
            combos = [f"{a} - {b} - {c}" for c in top[2:5]]
            amts = split_amount(budget_yen, len(combos))
            for c, aamt in zip(combos, amts):
                bets.append({"åˆ¸ç¨®": "ä¸‰é€£è¤‡", "è²·ã„ç›®": c, "é‡‘é¡(å††)": aamt})

    elif ticket_type == "ä¸‰é€£å˜":
        if len(top) >= 3:
            a, b = top[0], top[1]
            combos = [f"{a} â†’ {b} â†’ {c}" for c in top[2:5]]
            amts = split_amount(budget_yen, len(combos))
            for c, aamt in zip(combos, amts):
                bets.append({"åˆ¸ç¨®": "ä¸‰é€£å˜", "è²·ã„ç›®": c, "é‡‘é¡(å††)": aamt})

    return pd.DataFrame(bets)


# =========================
# UI
# =========================
JRA_PLACES = ["ä¸­å±±", "é˜ªç¥", "æ±äº¬", "äº¬éƒ½", "ä¸­äº¬", "å°å€‰", "æ–°æ½Ÿ", "ç¦å³¶", "æœ­å¹Œ", "å‡½é¤¨"]
NAR_PLACES = ["å¤§äº•", "å·å´"]

st.set_page_config(page_title="AIç«¶é¦¬äºˆæƒ³", layout="wide")
st.title("ğŸ AIç«¶é¦¬äºˆæƒ³")
st.caption("æ—¥ä»˜ãƒ»ç«¶é¦¬å ´ãƒ»Rç•ªå·ã‚’é¸ã‚“ã§æŠ¼ã™ã ã‘ã€‚é‡è¦–ãƒã‚¤ãƒ³ãƒˆã ã‘èª¿æ•´ã§ãã¾ã™ã€‚")

# ä¸Šéƒ¨åºƒå‘Šï¼ˆè¨­å®šã—ãŸå ´åˆã®ã¿è¡¨ç¤ºï¼‰
render_adsense(ADSENSE_SLOT_TOP, height=110)

with st.sidebar:
    st.header("ãƒ¬ãƒ¼ã‚¹å…¥åŠ›")
    race_date = st.date_input("æ—¥ä»˜", value=default_race_date())

    places = JRA_PLACES + NAR_PLACES
    place = st.selectbox("ç«¶é¦¬å ´", places)

    race_num = st.number_input("ãƒ¬ãƒ¼ã‚¹ç•ªå· (R)", min_value=1, max_value=12, value=11)

    with st.expander("ã†ã¾ãã„ã‹ãªã„æ™‚ï¼ˆä»»æ„ï¼‰"):
        entry_url_override = st.text_input("å‡ºé¦¬è¡¨URLã‚’è²¼ã‚‹ï¼ˆä»»æ„ï¼‰", value="")

    st.divider()
    st.header("é‡è¦–ãƒã‚¤ãƒ³ãƒˆï¼ˆåˆè¨ˆã¯è‡ªå‹•èª¿æ•´ï¼‰")
    w_ability = st.slider("èƒ½åŠ›ï¼ˆå®Ÿç¸¾ãƒ»ã‚¯ãƒ©ã‚¹ï¼‰", 0, 100, 40, 1)
    w_fit = st.slider("é©æ€§ï¼ˆè·é›¢ãƒ»é¦¬å ´ãƒ»ã‚³ãƒ¼ã‚¹ï¼‰", 0, 100, 25, 1)
    w_pace = st.slider("å±•é–‹ï¼ˆè„šè³ªãƒ»ãƒšãƒ¼ã‚¹ï¼‰", 0, 100, 20, 1)
    w_form = st.slider("èª¿å­ï¼ˆèª¿æ•™ãƒ»æ°—é…ï¼‰", 0, 100, 15, 1)
    w_cert = st.slider("ç¢ºåº¦ï¼ˆæƒ…å ±ãŒæƒã£ã¦ã„ã‚‹ã‹ï¼‰", 0, 100, 20, 1)

    st.divider()
    st.header("è²·ã„ç›®ï¼ˆä»»æ„ï¼‰")
    ticket_type = st.selectbox("åˆ¸ç¨®", ["é¦¬é€£", "ãƒ¯ã‚¤ãƒ‰", "é¦¬å˜", "ä¸‰é€£è¤‡", "ä¸‰é€£å˜", "å˜å‹", "è¤‡å‹"], index=0)
    budget_yen = st.number_input("äºˆç®—ï¼ˆå††ï¼‰", min_value=0, step=100, value=1000)

    invalid_jra_day = (place in JRA_PLACES) and (race_date.weekday() not in (5, 6))
    if invalid_jra_day:
        st.warning("JRAï¼ˆä¸­å¤®ï¼‰ã¯åŸºæœ¬åœŸæ—¥é–‹å‚¬ã§ã™ã€‚åœŸæ—¥ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")

    run_btn = st.button("AIäºˆæƒ³ã‚’ä½œæˆ", type="primary", disabled=invalid_jra_day)

if run_btn:
    date_str = race_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
    rn = f"{race_num}R"
    query = f"{date_str} {place} {rn} å‡ºé¦¬è¡¨ æ é †"

    st.subheader(f"ğŸ¯ å¯¾è±¡: {date_str} {place} {rn}")

    weights = normalize_weights({
        "èƒ½åŠ›": float(w_ability),
        "é©æ€§": float(w_fit),
        "å±•é–‹": float(w_pace),
        "èª¿å­": float(w_form),
        "ç¢ºåº¦": float(w_cert),
    })

    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµŒéæ™‚é–“ï¼ˆæ•´æ•°ã®ã¿ï¼‰
    time_box = st.empty()
    status_box = st.empty()
    start = time.perf_counter()

    def set_elapsed(prefix: str = "â± çµŒéæ™‚é–“"):
        elapsed = int(time.perf_counter() - start)
        time_box.info(f"{prefix}: {elapsed} ç§’")

    import concurrent.futures as cf

    weights_key = weights_to_key(weights)

    def job():
        return run_prediction_cached(query=query, weights_key=weights_key, entry_url_override=entry_url_override)

    status_box.write("AIãŒæƒ…å ±åé›†ãƒ»åˆ†æã—ã¦ã„ã¾ã™â€¦")
    with cf.ThreadPoolExecutor(max_workers=1) as ex:
        fut = ex.submit(job)
        while not fut.done():
            set_elapsed()
            time.sleep(0.2)

        try:
            df, sources, entry_url = fut.result()
        except Exception as e:
            set_elapsed("âœ– å¤±æ•—ï¼ˆçµŒéæ™‚é–“ï¼‰")
            if "ENTRYãƒ–ãƒ­ãƒƒã‚¯" in str(e):
                st.error("å‡ºé¦¬è¡¨ãŒå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã¾ãšæ—¥ä»˜ï¼ˆé–‹å‚¬æ—¥ï¼‰ã‚’ç¢ºèªã€‚ãƒ€ãƒ¡ãªã‚‰ã€Œå‡ºé¦¬è¡¨URLã‚’è²¼ã‚‹ã€ã‚’ä½¿ã†ã®ãŒæœ€çŸ­ã§ã™ã€‚")
            elif isinstance(e, genai_errors.ClientError) and is_quota_zero_message(e):
                st.error("Gemini API ã®åˆ©ç”¨æ ãŒ 0 ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆBilling/ãƒ—ãƒ©ãƒ³è¨­å®šï¼‰ã€‚Googleå´ã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error(str(e))
            st.exception(e)
            st.stop()

    set_elapsed("âœ… å®Œäº†ï¼ˆçµŒéæ™‚é–“ï¼‰")
    status_box.empty()

    # è¦‹ãŸç›®æ”¹å–„ï¼šæœ¬å‘½ã‚’ä¸Šã«å‡ºã™
    if not df.empty:
        top = df.iloc[0]
        st.metric("æœ¬å‘½ï¼ˆæš«å®šï¼‰", f"{int(top['é¦¬ç•ª'])}ç•ª {top['é¦¬å']}", f"ç·åˆç‚¹ {top['ç·åˆç‚¹']} / è©•ä¾¡ {top['è©•ä¾¡']}")

    st.success("äºˆæƒ³ã‚’ä½œæˆã—ã¾ã—ãŸï¼")

    st.markdown("### ğŸ“Œ å‡ºé¦¬è¡¨ã®å‚ç…§URL")
    if entry_url:
        st.markdown(f"â¡ï¸ [å‡ºé¦¬è¡¨ã‚’é–‹ã]({entry_url})")
    else:
        st.caption("å‚ç…§URLãŒå–ã‚Œãªã„å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚")

    st.markdown("### ğŸ“Š æœ‰åŠ›é¦¬ï¼ˆ5ã€œ6é ­ï¼‰")
    show_cols = [
        "æ ç•ª", "é¦¬ç•ª", "é¦¬å",
        "è©•ä¾¡", "ç·åˆç‚¹", "äººæ°—æƒ³å®š",
        "èƒ½åŠ›", "é©æ€§", "å±•é–‹", "èª¿å­", "ä¸ç¢ºå®Ÿæ€§",
        "æ€è€ƒã«ã‚ˆã‚‹åˆ†æçµæœ", "æ‡¸å¿µç‚¹"
    ]
    st.dataframe(df[show_cols], use_container_width=True)

    # CSV
    csv = df[show_cols].to_csv(index=False).encode("utf-8_sig")
    st.download_button("CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name=f"{place}_{race_num}R_{race_date:%Y%m%d}_ai.csv", mime="text/csv")

    st.markdown("---")
    st.markdown("### ğŸ’° è²·ã„ç›®ï¼ˆå‚è€ƒï¼‰")
    bets_df = make_bets(df_ranked=df, ticket_type=ticket_type, budget_yen=int(budget_yen))
    if bets_df.empty:
        st.info("è²·ã„ç›®ã¯æœªä½œæˆã§ã™ï¼ˆäºˆç®—0å†† or ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰ã€‚")
    else:
        st.dataframe(bets_df, use_container_width=True)

    with st.expander("ğŸ” å‚è€ƒã‚½ãƒ¼ã‚¹ï¼ˆå–å¾—ã§ããŸå ´åˆï¼‰"):
        if sources:
            for s in sources[:10]:
                st.write(f"- {s.get('title','')}: {s.get('uri','')}")
        else:
            st.caption("ã‚½ãƒ¼ã‚¹ãŒå–å¾—ã§ããªã„å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚")

    # ä¸‹éƒ¨åºƒå‘Šï¼ˆè¨­å®šã—ãŸå ´åˆã®ã¿è¡¨ç¤ºï¼‰
    render_adsense(ADSENSE_SLOT_BOTTOM, height=140)

else:
    st.info("å·¦ã®å…¥åŠ›ã‚’åŸ‹ã‚ã¦ã€ŒAIäºˆæƒ³ã‚’ä½œæˆã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
